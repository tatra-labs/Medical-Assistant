{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77880a79",
   "metadata": {},
   "source": [
    "### Medical Assistant Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb98992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(root_dir)\n",
    "\n",
    "original_dataset_path = os.path.join(root_dir, 'data', 'mle_screening_dataset.csv')\n",
    "print(original_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e009856",
   "metadata": {},
   "source": [
    "### Dataset Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.read_csv(original_dataset_path)\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates() \n",
    "\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7de781",
   "metadata": {},
   "source": [
    "#### Dataset Inspect Result\n",
    "\n",
    "This medical Question-Answer dataset contains 16,406 pairs of (question, answer). \n",
    "\n",
    "1. **Questions focus on definitions (e.g. What is (are) ..., ), symptoms (e.g. What are the symptoms of ...), prevention (How to prevent ...) and treatments (What are the treatments for ...) of diseases such as Glaucoma, High Blood Pressure, Tuberculosis, Cyclic Vomiting Syndrome, ... .**\n",
    "\n",
    "2. **There are several pairs containing the same question.** \n",
    "    * What is (are) Glaucoma ?\n",
    "    * What is (are) Cyclic Vomiting Syndrome ?\n",
    "    * ...\n",
    "\n",
    "3. **How well each answer addresses the question?** \n",
    "\n",
    "    The question is What is (are) Glaucoma ?\n",
    "    \n",
    "    There are three answer provided in the dataset. (I will rate each answer in the scale of 1 to 10.)\n",
    "    * **First answer:**\n",
    "        + Glaucoma is a group of diseases that can damage the eye's optic nerve and result in vision loss and blindness. The most common form of the disease is open-angle glaucoma. With early treatment, you can often protect your eyes against serious vision loss. (Watch the video to learn more about glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.)  See this graphic for a quick overview of glaucoma, including how many people it affects, whos at risk, what to do if you have it, and how to learn more.  See a glossary of glaucoma terms. \n",
    "        + Looks like answer was scraped from the website. It contains noise such as *(Watch the video to learn more about glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.)*\n",
    "        + Even though it contains noise, it address the glaucoma well. \n",
    "        + 8\n",
    "\n",
    "    * **Second Answer**\n",
    "        + The optic nerve is a bundle of more than 1 million nerve fibers. It connects the retina to the brain.\n",
    "        + This is obviously not the definition of glaucoma but optic nerve. \n",
    "        + 1\n",
    "\n",
    "    * **Third Answer**\n",
    "        + Open-angle glaucoma is the most common form of glaucoma. In the normal eye, the clear fluid leaves the anterior chamber at the open angle where the cornea and iris meet. When the fluid reaches the angle, it flows through a spongy meshwork, like a drain, and leaves the eye. Sometimes, when the fluid reaches the angle, it passes too slowly through the meshwork drain, causing the pressure inside the eye to build. If the pressure damages the optic nerve, open-angle glaucoma -- and vision loss -- may result.\n",
    "        + It addres open-angle glaucoma well, not glaucoma in general. \n",
    "        + 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af366c9",
   "metadata": {},
   "source": [
    "### Analysis \n",
    "\n",
    "1. Dataset Source  \n",
    "\n",
    "    (Assumption 1) I think current QA dataset contains question-aanswer pairs from MedQuAD (a public dataset that includes 47,457 medical question-answer pairs created from 12 NIH websites (e.g. cancer.gov, niddk.nih.gov, GARD, MedlinePlus Health Topics). The collection covers 37 question types (e.g. Treatment, Diagnosis, Side Effects) associated with diseases, drugs and other medical entities such as tests..). \n",
    "\n",
    "    To be accurate, we need to determine each QA pair from the current dataset and combine two datasets into one but based on the assumption I will use MedQuAD only here.\n",
    "    `data/MedQuad_QA.csv` which is 24 MB is not good. So I will use clean MedQuAD dataset (`data/clean_data.csv`). This is another assumption here. \n",
    "\n",
    "2. Dataset Enrichment \n",
    "\n",
    "    I think it is useful to visit this [url](https://github.com/abachaa/Existing-Medical-QA-Datasets) \n",
    "    Let's keep in mind that this [repo](https://github.com/mingzhu0527/MASHQA?tab=readme-ov-file) could enrich our dataset more if we want to improve solution later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4937c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "medquad_dataset_path = os.path.join(root_dir, 'data', 'clean_data.csv')\n",
    "medquad_df = pd.read_csv(medquad_dataset_path)\n",
    "\n",
    "medquad_df = medquad_df.dropna() \n",
    "medquad_df = medquad_df.drop_duplicates()\n",
    "\n",
    "print(medquad_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021456f",
   "metadata": {},
   "source": [
    "### Model Selection \n",
    "\n",
    "I chose SciFive for this medical question-answering task.\n",
    "\n",
    "- Why? \n",
    "    * SciFive is based on T5 architecture and pre-trained on extensive medical literature(PubMed, PMC), ensuring it understands medical terminology and concepts.\n",
    "    * This is a strong choice for question-answering. \n",
    "    * Model parameters are 770M which could be a good fit for my GPU RAM size(48GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "base_model_path = os.path.join(root_dir, 'models', 'SciFive-large-Pubmed_PMC')\n",
    "\n",
    "# Load SciFive\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf8b08",
   "metadata": {},
   "source": [
    "Based on tokenizer and max token length, I sanitized dataset. \n",
    "\n",
    "Total tokens: 4,737,589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b63cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_dataset_path = os.path.join(root_dir, 'data', 'sanitized_data.csv')\n",
    "sanitized_df = pd.read_csv(sanitized_dataset_path)\n",
    "print(sanitized_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c803eb3",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn as nn \n",
    "\n",
    "class MedQuQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=1024):\n",
    "        self.df = df \n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_length = max_length \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        question, answer = self.df.iloc[idx]['prompt'], self.df.iloc[idx]['response']\n",
    "        inputs = self.tokenizer(\n",
    "            f\"question: {question}\",\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            answer,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels['input_ids'].squeeze()\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40e6a2",
   "metadata": {},
   "source": [
    "### Model Fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric \n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "results_path = os.path.join(root_dir, 'results')\n",
    "logs_path = os.path.join(root_dir, 'logs')\n",
    "\n",
    "train_val_df, test_df = train_test_split(sanitized_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MedQuQADataset(train_df, tokenizer)\n",
    "val_dataset = MedQuQADataset(val_df, tokenizer)\n",
    "test_dataset = MedQuQADataset(test_df, tokenizer)\n",
    "\n",
    "# Metrics \n",
    "f1_metric = load_metric(\"f1\", trust_remote_code=True)\n",
    "bleu_metric = load_metric(\"bleu\", trust_remote_code=True)\n",
    "rouge_metric = load_metric(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred \n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True) \n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) \n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) \n",
    "    \n",
    "    # Normalize predictions and labels for EM (strip whitespace)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    # Exact Match (EM)\n",
    "    em_scores = [1 if pred == ref else 0 for pred, ref in zip(decoded_preds, decoded_labels)]\n",
    "    em_result = {\"exact_match\": np.mean(em_scores)}\n",
    "    \n",
    "    # BLEU \n",
    "    bleu_preds = [pred.split() for pred in decoded_preds]\n",
    "    bleu_refs = [[ref.split()] for ref in decoded_labels]\n",
    "    bleu_result = bleu_metric.compute(predictions=bleu_preds, references=bleu_refs)\n",
    "    \n",
    "    # F1 \n",
    "    f1_preds = [set(pred.split()) for pred in decoded_preds]\n",
    "    f1_refs = [set(ref.split()) for ref in decoded_labels]\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(f1_preds, f1_refs):\n",
    "        true_positives = len(pred & ref)\n",
    "        precision = true_positives / len(pred) if pred else 0\n",
    "        recall = true_positives / len(ref) if ref else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "        f1_scores.append(f1)\n",
    "    f1_result = {\"f1\": np.mean(f1_scores)}\n",
    "    \n",
    "    # ROUGE \n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": em_result[\"exact_match\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"],\n",
    "        \"f1\": f1_result[\"f1\"],\n",
    "        \"rouge1\": rouge_result[\"rouge1\"].mid.fmeasure,\n",
    "        \"rouge2\": rouge_result[\"rouge2\"].mid.fmeasure,\n",
    "        \"rougeL\": rouge_result[\"rougeL\"].mid.fmeasure\n",
    "    }\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_path,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=logs_path,\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,  # Evaluate every 10 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,  # Checkpoint every 10 steps\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset) \n",
    "\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
